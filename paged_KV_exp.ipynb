{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a5ecf82-99ca-4118-bdde-167d1b7e5bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "GPU: NVIDIA RTX 4000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d34321b9-aaf3-49c7-8eae-983e95689e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    vocab_size = 32000\n",
    "    hidden_size = 768\n",
    "    num_heads = 12\n",
    "    num_layers = 6\n",
    "    max_seq_len = 32768  # allow long contexts\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77a70dbc-15c6-4ab4-9813-889c0c489ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paged KV cache\n",
    "\n",
    "class PagedKVCache:\n",
    "    def __init__(self, batch_size, num_heads, head_dim,\n",
    "                 page_size=256, device=\"cuda\"):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.page_size = page_size\n",
    "        self.device = device\n",
    "\n",
    "        # List of allocated pages\n",
    "        self.k_pages = []\n",
    "        self.v_pages = []\n",
    "\n",
    "        self.cur_pos = 0  # total tokens stored\n",
    "\n",
    "    def _allocate_page(self):\n",
    "        k_page = torch.zeros(\n",
    "            self.batch_size,\n",
    "            self.num_heads,\n",
    "            self.page_size,\n",
    "            self.head_dim,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        v_page = torch.zeros_like(k_page)\n",
    "\n",
    "        self.k_pages.append(k_page)\n",
    "        self.v_pages.append(v_page)\n",
    "\n",
    "    def update(self, new_k, new_v):\n",
    "        B, H, T, D = new_k.shape\n",
    "\n",
    "        for t in range(T):\n",
    "            page_idx = self.cur_pos // self.page_size\n",
    "            offset = self.cur_pos % self.page_size\n",
    "\n",
    "            if page_idx >= len(self.k_pages):\n",
    "                self._allocate_page()\n",
    "\n",
    "            self.k_pages[page_idx][:, :, offset, :] = new_k[:, :, t, :]\n",
    "            self.v_pages[page_idx][:, :, offset, :] = new_v[:, :, t, :]\n",
    "\n",
    "            self.cur_pos += 1\n",
    "\n",
    "        # Return concatenated view\n",
    "        k = torch.cat(self.k_pages, dim=2)[:, :, :self.cur_pos, :]\n",
    "        v = torch.cat(self.v_pages, dim=2)[:, :, :self.cur_pos, :]\n",
    "\n",
    "        return k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2eebc22-ff09-439c-8131-2c3ac9b98fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageStreamingAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            kv_cache.update(k, v)\n",
    "\n",
    "        # Streaming attention over pages\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "        output = torch.zeros_like(q)\n",
    "\n",
    "        for page_idx in range(len(kv_cache.k_pages)):\n",
    "            k_page = kv_cache.k_pages[page_idx]\n",
    "            v_page = kv_cache.v_pages[page_idx]\n",
    "\n",
    "            # Only consider valid tokens in last page\n",
    "            if page_idx == len(kv_cache.k_pages) - 1:\n",
    "                valid_tokens = kv_cache.cur_pos % kv_cache.page_size\n",
    "                if valid_tokens == 0:\n",
    "                    valid_tokens = kv_cache.page_size\n",
    "                k_page = k_page[:, :, :valid_tokens, :]\n",
    "                v_page = v_page[:, :, :valid_tokens, :]\n",
    "\n",
    "            scores = torch.matmul(q, k_page.transpose(-2, -1)) * scale\n",
    "            attn = torch.softmax(scores, dim=-1)\n",
    "            output += torch.matmul(attn, v_page)\n",
    "\n",
    "        output = output.transpose(1,2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95f318a0-5892-4aef-b3cd-11f1061f4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedKVCache:\n",
    "    def __init__(self, batch_size, num_heads, head_dim,\n",
    "                 page_size=256, device=\"cuda\"):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.page_size = page_size\n",
    "        self.device = device\n",
    "\n",
    "        self.k_pages = []\n",
    "        self.v_pages = []\n",
    "        self.cur_pos = 0\n",
    "\n",
    "    def _allocate_page(self):\n",
    "        k_page = torch.zeros(\n",
    "            self.batch_size,\n",
    "            self.num_heads,\n",
    "            self.page_size,\n",
    "            self.head_dim,\n",
    "            device=self.device\n",
    "        )\n",
    "        v_page = torch.zeros_like(k_page)\n",
    "\n",
    "        self.k_pages.append(k_page)\n",
    "        self.v_pages.append(v_page)\n",
    "\n",
    "    def update(self, new_k, new_v):\n",
    "        B, H, T, D = new_k.shape\n",
    "\n",
    "        for t in range(T):\n",
    "            page_idx = self.cur_pos // self.page_size\n",
    "            offset = self.cur_pos % self.page_size\n",
    "\n",
    "            if page_idx >= len(self.k_pages):\n",
    "                self._allocate_page()\n",
    "\n",
    "            self.k_pages[page_idx][:, :, offset, :] = new_k[:, :, t, :]\n",
    "            self.v_pages[page_idx][:, :, offset, :] = new_v[:, :, t, :]\n",
    "\n",
    "            self.cur_pos += 1\n",
    "\n",
    "        # Concatenate only once per forward\n",
    "        k = torch.cat(self.k_pages, dim=2)[:, :, :self.cur_pos, :]\n",
    "        v = torch.cat(self.v_pages, dim=2)[:, :, :self.cur_pos, :]\n",
    "\n",
    "        return k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74272d3d-a707-45ca-9df5-8132d2e05ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attn = PageStreamingAttention(config.hidden_size, config.num_heads)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 4 * config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.hidden_size, config.hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        x = x + self.attn(self.ln1(x), kv_cache)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(config)\n",
    "            for _ in range(config.num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(config.hidden_size)\n",
    "        self.head = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, kv_caches=None):\n",
    "        x = self.embed(input_ids)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            cache = kv_caches[i] if kv_caches else None\n",
    "            x = layer(x, cache)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2febb127-2426-4a3d-b7c1-627315b1a51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready\n"
     ]
    }
   ],
   "source": [
    "model = MiniTransformer(config).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3fe6eb4-3873-4d16-a8ea-358bc2481816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"Allocated: {allocated:.2f} GB | Reserved: {reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bdccb05-a3e3-41f9-a571-49cc87b31a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefill_with_paging(seq_len):\n",
    "    input_ids = torch.randint(0, config.vocab_size,\n",
    "                              (1, seq_len),\n",
    "                              device=device)\n",
    "\n",
    "    kv_caches = [\n",
    "        PagedKVCache(\n",
    "            1,\n",
    "            config.num_heads,\n",
    "            config.hidden_size // config.num_heads,\n",
    "            page_size=256,\n",
    "            device=device\n",
    "        )\n",
    "        for _ in range(config.num_layers)\n",
    "    ]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory()\n",
    "\n",
    "    _ = model(input_ids, kv_caches)\n",
    "\n",
    "    print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c1efad0-f059-4469-bee5-923047cd78e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 1.10 GB | Reserved: 1.18 GB\n",
      "Allocated: 1.77 GB | Reserved: 2.20 GB\n",
      "Allocated: 1.10 GB | Reserved: 1.18 GB\n",
      "Allocated: 2.45 GB | Reserved: 3.21 GB\n",
      "Allocated: 1.10 GB | Reserved: 1.18 GB\n",
      "Allocated: 3.80 GB | Reserved: 5.24 GB\n"
     ]
    }
   ],
   "source": [
    "prefill_with_paging(4096)\n",
    "prefill_with_paging(8192)\n",
    "prefill_with_paging(16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75a9dd88-e920-44d9-9e95-7260b368413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefill_with_paging(seq_len):\n",
    "    input_ids = torch.randint(0, config.vocab_size,\n",
    "                              (1, seq_len),\n",
    "                              device=device)\n",
    "\n",
    "    kv_caches = [\n",
    "        PagedKVCache(\n",
    "            1,\n",
    "            config.num_heads,\n",
    "            config.hidden_size // config.num_heads,\n",
    "            page_size=256,\n",
    "            device=device\n",
    "        )\n",
    "        for _ in range(config.num_layers)\n",
    "    ]\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "\n",
    "    _ = model(input_ids, kv_caches)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Prefill latency: {end - start:.4f}s\")\n",
    "    print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7057a0ba-fdca-443f-95d1-9febd8041463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 1.10 GB | Reserved: 1.18 GB\n",
      "Prefill latency: 1.3224s\n",
      "Allocated: 1.77 GB | Reserved: 2.20 GB\n",
      "Allocated: 1.10 GB | Reserved: 1.18 GB\n",
      "Prefill latency: 2.8658s\n",
      "Allocated: 2.45 GB | Reserved: 3.21 GB\n",
      "Allocated: 1.10 GB | Reserved: 1.18 GB\n",
      "Prefill latency: 6.7294s\n",
      "Allocated: 3.80 GB | Reserved: 5.24 GB\n"
     ]
    }
   ],
   "source": [
    "prefill_with_paging(4096)\n",
    "prefill_with_paging(8192)\n",
    "prefill_with_paging(16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ff2c995-5c3e-44ad-adab-f9002cec0fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_with_paging(prefill_len, steps=256):\n",
    "    input_ids = torch.randint(0, config.vocab_size,\n",
    "                              (1, prefill_len),\n",
    "                              device=device)\n",
    "\n",
    "    kv_caches = [\n",
    "        PagedKVCache(\n",
    "            1,\n",
    "            config.num_heads,\n",
    "            config.hidden_size // config.num_heads,\n",
    "            page_size=256,\n",
    "            device=device\n",
    "        )\n",
    "        for _ in range(config.num_layers)\n",
    "    ]\n",
    "\n",
    "    # Prefill\n",
    "    _ = model(input_ids, kv_caches)\n",
    "\n",
    "    input_ids = torch.randint(0, config.vocab_size,\n",
    "                              (1, 1),\n",
    "                              device=device)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(steps):\n",
    "        logits = model(input_ids, kv_caches)\n",
    "        input_ids = torch.argmax(logits[:, -1, :],\n",
    "                                 dim=-1,\n",
    "                                 keepdim=True)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Decode tok/s: {steps / (end - start):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98c4ff48-88f8-4ede-a685-ba6499020664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode tok/s: 217.68\n",
      "Decode tok/s: 181.51\n",
      "Decode tok/s: 135.04\n",
      "Decode tok/s: 89.36\n"
     ]
    }
   ],
   "source": [
    "for ctx in [512, 1024, 2048, 4096]:\n",
    "    decode_with_paging(ctx, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d368ef7c-a8d4-40e2-af85-406cc3c9f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageStreamingAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        B, T, C = x.shape\n",
    "    \n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "    \n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "    \n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "    \n",
    "        # ðŸ”¹ Case 1: No KV cache â†’ standard full attention\n",
    "        if kv_cache is None:\n",
    "            attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "            attn = torch.softmax(attn, dim=-1)\n",
    "            out = torch.matmul(attn, v)\n",
    "            out = out.transpose(1,2).contiguous().view(B, T, C)\n",
    "            return self.out_proj(out)\n",
    "    \n",
    "        # ðŸ”¹ Case 2: With KV cache â†’ paged streaming\n",
    "    \n",
    "        kv_cache.update(k, v)\n",
    "    \n",
    "        output = torch.zeros_like(q)\n",
    "    \n",
    "        m_i = torch.full((B, self.num_heads, T),\n",
    "                         -float(\"inf\"), device=q.device)\n",
    "        l_i = torch.zeros((B, self.num_heads, T),\n",
    "                          device=q.device)\n",
    "    \n",
    "        for page_idx in range(len(kv_cache.k_pages)):\n",
    "            k_page = kv_cache.k_pages[page_idx]\n",
    "            v_page = kv_cache.v_pages[page_idx]\n",
    "    \n",
    "            if page_idx == len(kv_cache.k_pages) - 1:\n",
    "                valid = kv_cache.cur_pos % kv_cache.page_size\n",
    "                if valid != 0:\n",
    "                    k_page = k_page[:, :, :valid, :]\n",
    "                    v_page = v_page[:, :, :valid, :]\n",
    "    \n",
    "            scores = torch.matmul(q, k_page.transpose(-2, -1)) * scale\n",
    "    \n",
    "            block_max = scores.max(dim=-1).values\n",
    "            new_m = torch.maximum(m_i, block_max)\n",
    "    \n",
    "            exp_scores = torch.exp(scores - new_m.unsqueeze(-1))\n",
    "            exp_m_diff = torch.exp(m_i - new_m)\n",
    "    \n",
    "            l_i = exp_m_diff * l_i + exp_scores.sum(dim=-1)\n",
    "            output = exp_m_diff.unsqueeze(-1) * output + torch.matmul(exp_scores, v_page)\n",
    "    \n",
    "            m_i = new_m\n",
    "    \n",
    "        output = output / l_i.unsqueeze(-1)\n",
    "        output = output.transpose(1,2).contiguous().view(B, T, C)\n",
    "    \n",
    "        return self.out_proj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "214eb2df-4fa8-4649-aee4-9fe8558a7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_attention(seq_len=128):\n",
    "    input_ids = torch.randint(0, config.vocab_size,\n",
    "                              (1, seq_len),\n",
    "                              device=device)\n",
    "\n",
    "    # Create one base model\n",
    "    base_model = MiniTransformer(config).to(device)\n",
    "    base_model.eval()\n",
    "\n",
    "    # Clone it for streaming\n",
    "    streaming_model = MiniTransformer(config).to(device)\n",
    "    streaming_model.load_state_dict(base_model.state_dict())\n",
    "    streaming_model.eval()\n",
    "\n",
    "    # Replace attention in streaming model\n",
    "    for i in range(config.num_layers):\n",
    "        streaming_model.layers[i].attn = PageStreamingAttention(\n",
    "            config.hidden_size,\n",
    "            config.num_heads\n",
    "        ).to(device)\n",
    "\n",
    "    naive_out = base_model(input_ids)\n",
    "\n",
    "    kv_caches = [\n",
    "        PagedKVCache(\n",
    "            1,\n",
    "            config.num_heads,\n",
    "            config.hidden_size // config.num_heads,\n",
    "            page_size=256,\n",
    "            device=device\n",
    "        )\n",
    "        for _ in range(config.num_layers)\n",
    "    ]\n",
    "\n",
    "    streaming_out = streaming_model(input_ids, kv_caches)\n",
    "\n",
    "    diff = (naive_out - streaming_out).abs().max()\n",
    "    print(\"Max difference:\", diff.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e4020c7-5889-4486-be24-ab32374365e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference: 0.5442786812782288\n"
     ]
    }
   ],
   "source": [
    "compare_attention(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42303fa5-ca84-4836-808b-7d1b15cfd7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 2.22 GB | Reserved: 2.34 GB\n",
      "Prefill latency: 1.2965s\n",
      "Allocated: 2.90 GB | Reserved: 3.31 GB\n",
      "Allocated: 2.22 GB | Reserved: 2.34 GB\n",
      "Prefill latency: 2.8709s\n",
      "Allocated: 3.57 GB | Reserved: 4.28 GB\n",
      "Allocated: 2.22 GB | Reserved: 2.34 GB\n",
      "Prefill latency: 6.7553s\n",
      "Allocated: 4.92 GB | Reserved: 6.33 GB\n"
     ]
    }
   ],
   "source": [
    "prefill_with_paging(4096)\n",
    "prefill_with_paging(8192)\n",
    "prefill_with_paging(16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e7e5f41-a53b-46af-9533-f38bf85dac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode tok/s: 219.90\n",
      "Decode tok/s: 181.48\n",
      "Decode tok/s: 134.79\n",
      "Decode tok/s: 88.98\n"
     ]
    }
   ],
   "source": [
    "for ctx in [512, 1024, 2048, 4096]:\n",
    "    decode_with_paging(ctx, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5dfa3-8c9f-4365-8656-ca457132da83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
