{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad967aad-0b8b-4101-8339-5f30815939e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "GPU: NVIDIA RTX 4000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bad4af0a-7b47-41da-b4d4-101994075b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets do the model config\n",
    "\n",
    "class Config:\n",
    "    vocab_size = 32000\n",
    "    hidden_size = 768\n",
    "    num_heads = 12\n",
    "    num_layers = 6\n",
    "    max_seq_len = 32768\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8deaa99d-b6c7-4a41-960d-4052d1e37492",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Naive Attention with KV cache\n",
    "\n",
    "class NaiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        self.q_proj   =   nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj   =   nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj   =   nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj =   nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        ## this is where we update our KV cache\n",
    "        if kv_cache is not None:\n",
    "            k, v = kv_cache.update(k, v)\n",
    "\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.out_proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73da1b93-d69e-4d00-8939-a2f350eaa4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## KV Cache\n",
    "\n",
    "class KVCache:\n",
    "    def __init__(self, batch_size, num_heads, max_seq_len, head_dim):\n",
    "        self.k = torch.zeros(batch_size, num_heads, max_seq_len, head_dim, device=device)\n",
    "        self.v = torch.zeros(batch_size, num_heads, max_seq_len, head_dim, device=device)\n",
    "        self.cur_pos = 0\n",
    "\n",
    "    def update(self, new_k, new_v):\n",
    "        B, H, T, D = new_k.shape\n",
    "\n",
    "        self.k[:, :, self.cur_pos:self.cur_pos+T, :] = new_k\n",
    "        self.v[:, :, self.cur_pos:self.cur_pos+T, :] = new_v\n",
    "        self.cur_pos += T\n",
    "\n",
    "        return (\n",
    "            self.k[:, :, :self.cur_pos, :],\n",
    "            self.v[:, :, :self.cur_pos, :]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d8f00be-c0d1-48b5-a337-db9b679d1fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attn = NaiveAttention(config.hidden_size, config.num_heads)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 4 * config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.hidden_size, config.hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        x = x + self.attn(self.ln1(x), kv_cache)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(config)\n",
    "            for _ in range(config.num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(config.hidden_size)\n",
    "        self.head = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, kv_caches=None):\n",
    "        x = self.embed(input_ids)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            cache = kv_caches[i] if kv_caches else None\n",
    "            x = layer(x, cache)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28c84b89-e230-42f3-9eeb-fe94a656ec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 91.712768 M\n"
     ]
    }
   ],
   "source": [
    "## Instatiate model\n",
    "\n",
    "model = MiniTransformer(config).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model parameters:\",\n",
    "      sum(p.numel() for p in model.parameters()) / 1e6, \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c960700-433f-4594-b6f4-345607d91a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill latency: 0.1606s\n"
     ]
    }
   ],
   "source": [
    "def benchmark_prefill(seq_len=1024, batch_size=1):\n",
    "    input_ids = torch.randint(0, config.vocab_size,\n",
    "                              (batch_size, seq_len),\n",
    "                              device=device)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    _ = model(input_ids)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Prefill latency: {end-start:.4f}s\")\n",
    "\n",
    "benchmark_prefill(1024, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b61ee0b-2121-4df8-822b-c2f0cb70f045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode tokens/sec: 323.65\n"
     ]
    }
   ],
   "source": [
    "## decode benchmark\n",
    "\n",
    "def benchmark_decode(steps=128, batch_size=1):\n",
    "    input_ids = torch.randint(0, config.vocab_size,\n",
    "                              (batch_size, 1),\n",
    "                              device=device)\n",
    "\n",
    "    kv_caches = [\n",
    "        KVCache(\n",
    "            batch_size,\n",
    "            config.num_heads,\n",
    "            config.max_seq_len,\n",
    "            config.hidden_size // config.num_heads\n",
    "        )\n",
    "        for _ in range(config.num_layers)\n",
    "    ]\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(steps):\n",
    "        logits = model(input_ids, kv_caches)\n",
    "        input_ids = torch.argmax(logits[:, -1, :],\n",
    "                                 dim=-1,\n",
    "                                 keepdim=True)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Decode tokens/sec: {steps / (end - start):.2f}\")\n",
    "\n",
    "benchmark_decode(256, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35a78e8a-66d2-4348-814d-9d8ad42a3710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill latency: 0.0064s\n",
      "Prefill latency: 0.0060s\n",
      "Prefill latency: 0.0104s\n",
      "Prefill latency: 0.0210s\n",
      "Prefill latency: 0.1097s\n",
      "Prefill latency: 0.3973s\n"
     ]
    }
   ],
   "source": [
    "## prefix scaling\n",
    "\n",
    "for seq in [256, 512, 1024, 1536, 4048, 8026]:\n",
    "    benchmark_prefill(seq, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c86a0a1-f418-4a74-95eb-7e5a3bf43ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode tokens/sec: 349.10\n",
      "Decode tokens/sec: 351.18\n",
      "Decode tokens/sec: 350.37\n",
      "Decode tokens/sec: 348.15\n"
     ]
    }
   ],
   "source": [
    "## decode scaling\n",
    "for steps in [128, 256, 512, 1024]:\n",
    "    benchmark_decode(steps, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e20ebbcc-bc95-45d1-8247-197b815b8523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.38 GB | Reserved: 0.41 GB\n",
      "Prefill latency: 0.1359s\n",
      "Allocated: 0.38 GB | Reserved: 2.08 GB\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_memory():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"Allocated: {allocated:.2f} GB | Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_memory()\n",
    "benchmark_prefill(4096, 1)\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfe2c9bd-3837-4221-9d58-ee598e66de5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill latency: 0.4162s\n",
      "Allocated: 0.38 GB | Reserved: 8.52 GB\n"
     ]
    }
   ],
   "source": [
    "benchmark_prefill(8192, 1)\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "580c1e99-06d8-4961-9c53-792e4c71f823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.38 GB | Reserved: 0.41 GB\n",
      "Prefill latency: 0.4851s\n",
      "Allocated: 0.38 GB | Reserved: 7.20 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print_gpu_memory()\n",
    "benchmark_prefill(4096, 4)\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcac3a55-9f00-4219-8556-9cf4246852a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FlashAttention\n",
    "\n",
    "class BlockedAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, block_size=256):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "        output = torch.zeros_like(q)\n",
    "\n",
    "        for start in range(0, T, self.block_size):\n",
    "            end = min(start + self.block_size, T)\n",
    "\n",
    "            q_block = q[:, :, start:end, :]  # (B,H,block,D)\n",
    "\n",
    "            # Online softmax components\n",
    "            m_i = torch.full((B, self.num_heads, end-start), \n",
    "                             -float(\"inf\"), device=q.device)\n",
    "            l_i = torch.zeros((B, self.num_heads, end-start), \n",
    "                              device=q.device)\n",
    "            acc = torch.zeros_like(q_block)\n",
    "\n",
    "            for k_start in range(0, T, self.block_size):\n",
    "                k_end = min(k_start + self.block_size, T)\n",
    "\n",
    "                k_block = k[:, :, k_start:k_end, :]\n",
    "                v_block = v[:, :, k_start:k_end, :]\n",
    "\n",
    "                scores = torch.matmul(q_block, k_block.transpose(-2,-1)) * scale\n",
    "\n",
    "                block_max = scores.max(dim=-1).values\n",
    "                new_m = torch.maximum(m_i, block_max)\n",
    "\n",
    "                exp_scores = torch.exp(scores - new_m.unsqueeze(-1))\n",
    "                exp_m_diff = torch.exp(m_i - new_m)\n",
    "\n",
    "                l_i = exp_m_diff * l_i + exp_scores.sum(dim=-1)\n",
    "                acc = exp_m_diff.unsqueeze(-1) * acc + torch.matmul(exp_scores, v_block)\n",
    "\n",
    "                m_i = new_m\n",
    "\n",
    "            output[:, :, start:end, :] = acc / l_i.unsqueeze(-1)\n",
    "\n",
    "        output = output.transpose(1,2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.out_proj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cb8f90c-d811-4dbc-a4b1-250590de081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockedTransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attn = BlockedAttention(config.hidden_size, config.num_heads, block_size=256)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 4 * config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.hidden_size, config.hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class BlockedTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            BlockedTransformerBlock(config)\n",
    "            for _ in range(config.num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(config.hidden_size)\n",
    "        self.head = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3c1dea1-5bd6-492f-b277-ff49c8bd8680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlockedTransformer(\n",
       "  (embed): Embedding(32000, 768)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x BlockedTransformerBlock(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): BlockedAttention(\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=768, out_features=32000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocked_model = BlockedTransformer(config).to(device)\n",
    "blocked_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdd2eb6a-86b9-45ac-ae1c-54906a951060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_blocked(seq_len=4096, batch_size=1):\n",
    "    input_ids = torch.randint(0, config.vocab_size,\n",
    "                              (batch_size, seq_len),\n",
    "                              device=device)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    _ = blocked_model(input_ids)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Blocked prefill latency: {end-start:.4f}s\")\n",
    "    print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1f1025c-0bf4-4a7d-81a9-c1e095a1c526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.75 GB | Reserved: 3.83 GB\n",
      "Blocked prefill latency: 0.3588s\n",
      "Allocated: 1.27 GB | Reserved: 3.84 GB\n",
      "Allocated: 0.75 GB | Reserved: 3.83 GB\n",
      "Blocked prefill latency: 20.9284s\n",
      "Allocated: 4.84 GB | Reserved: 7.93 GB\n"
     ]
    }
   ],
   "source": [
    "benchmark_blocked(4096, 1)\n",
    "benchmark_blocked(32000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90682d44-9641-42a0-8669-41cd2a744765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_after_prefill(prefill_len, steps=256):\n",
    "    input_ids = torch.randint(0, config.vocab_size,\n",
    "                              (1, prefill_len),\n",
    "                              device=device)\n",
    "\n",
    "    kv_caches = [\n",
    "        KVCache(\n",
    "            1,\n",
    "            config.num_heads,\n",
    "            config.max_seq_len,\n",
    "            config.hidden_size // config.num_heads\n",
    "        )\n",
    "        for _ in range(config.num_layers)\n",
    "    ]\n",
    "\n",
    "    # Prefill\n",
    "    _ = model(input_ids, kv_caches)\n",
    "\n",
    "    input_ids = torch.randint(0, config.vocab_size,\n",
    "                              (1, 1),\n",
    "                              device=device)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(steps):\n",
    "        logits = model(input_ids, kv_caches)\n",
    "        input_ids = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Prefill {prefill_len} → decode tok/s:\",\n",
    "          steps / (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a4cb996-e9e5-48a7-94ff-9c7a09d12d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill 512 → decode tok/s: 337.76002576904494\n",
      "Prefill 1024 → decode tok/s: 350.66236060350633\n",
      "Prefill 2048 → decode tok/s: 345.42498582902306\n",
      "Prefill 4096 → decode tok/s: 351.57389214498545\n"
     ]
    }
   ],
   "source": [
    "for ctx in [512, 1024, 2048, 4096]:\n",
    "    decode_after_prefill(ctx, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ba9c4b5-65f9-4713-8e22-b987d8fde787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_generation_time(total_tokens):\n",
    "    input_ids = torch.randint(0, config.vocab_size,\n",
    "                              (1, 1),\n",
    "                              device=device)\n",
    "\n",
    "    kv_caches = [\n",
    "        KVCache(\n",
    "            1,\n",
    "            config.num_heads,\n",
    "            config.max_seq_len,\n",
    "            config.hidden_size // config.num_heads\n",
    "        )\n",
    "        for _ in range(config.num_layers)\n",
    "    ]\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "\n",
    "    for _ in range(total_tokens):\n",
    "        logits = model(input_ids, kv_caches)\n",
    "        input_ids = torch.argmax(logits[:, -1, :],\n",
    "                                 dim=-1,\n",
    "                                 keepdim=True)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Generate {total_tokens} tokens → {end-start:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c193764-7da7-4fab-8e26-4bcd8addd042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate 256 tokens → 0.7628s\n",
      "Generate 512 tokens → 1.4797s\n",
      "Generate 1024 tokens → 3.0131s\n"
     ]
    }
   ],
   "source": [
    "for t in [256, 512, 1024]:\n",
    "    full_generation_time(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "076e8f1c-f8c2-4c80-8374-e6e94d7c8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## KV cache memory scaling\n",
    "\n",
    "def kv_cache_memory(batch, seq_len):\n",
    "    bytes_per_element = 2  # assuming FP16\n",
    "    total = (\n",
    "        batch *\n",
    "        config.num_layers *\n",
    "        config.num_heads *\n",
    "        seq_len *\n",
    "        (config.hidden_size // config.num_heads) *\n",
    "        2 *  # K and V\n",
    "        bytes_per_element\n",
    "    )\n",
    "    print(f\"Approx KV cache size: {total / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a3522bb-3cb5-4d4d-829f-a6fdbe057755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx KV cache size: 0.08 GB\n",
      "Approx KV cache size: 0.15 GB\n",
      "Approx KV cache size: 0.60 GB\n"
     ]
    }
   ],
   "source": [
    "kv_cache_memory(1, 4096)\n",
    "kv_cache_memory(1, 8192)\n",
    "kv_cache_memory(4, 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "821cc431-892c-4069-b91d-75d6ae2875d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6f99171-66a5-47d4-a75b-f24d655d6dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a395ce7e-cc03-4c28-9a67-3a885e7080a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.88 GB | Reserved: 3.83 GB\n",
      "Prefill 4096 → decode tok/s: 0.0\n",
      "Allocated: 0.88 GB | Reserved: 3.83 GB\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4cf81b8f-054e-4ffb-ab14-4c82c337235f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.88 GB | Reserved: 3.83 GB\n",
      "Prefill 8192 → decode tok/s: 0.0\n",
      "Allocated: 0.88 GB | Reserved: 10.27 GB\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3eeeb3-01c8-47e5-b869-f63c768c651f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
